{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxjGiug1DmSI"
   },
   "outputs": [],
   "source": [
    "%pip install mambapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3024,
     "status": "ok",
     "timestamp": 1734936531480,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": -480
    },
    "id": "pEzrz04BDK03",
    "outputId": "12af59dc-1304-4fd1-a45f-650781252ca8"
   },
   "outputs": [],
   "source": [
    "import torch, transformers, importlib\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import Mamba, MambaVAE, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "inputs = tokenizer([\"Today is Sunday.\", \"Today is.\"], \n",
    "                      return_tensors=\"pt\", \n",
    "                      truncation=True, \n",
    "                      max_length=10, \n",
    "                      padding=\"max_length\") #.to('cuda')\n",
    "ones = torch.ones((inputs['attention_mask'].size(0), 1))\n",
    "inputs['attention_mask'] = torch.cat([ones, inputs['attention_mask'][:, :-1]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(27.2230, grad_fn=<DivBackward0>),\n",
       " tensor(40509.1094, grad_fn=<MulBackward0>),\n",
       " tensor([[[ 15.4702,  -6.4229,  11.7433,  ...,  -6.3040,  -6.8389,  -6.4293],\n",
       "          [ 13.2942, -49.8134,   4.4575,  ..., -49.5522, -50.0998, -49.6902],\n",
       "          [  9.3128, -42.6583,   0.5586,  ..., -42.3826, -42.9728, -42.5657],\n",
       "          ...,\n",
       "          [  3.7884, -17.8751,   6.2833,  ..., -18.5168, -18.0260, -18.3387],\n",
       "          [  2.8193, -15.6524,   6.6504,  ..., -16.2095, -15.6207, -16.1042],\n",
       "          [  0.9937, -11.9810,   6.8434,  ..., -12.4709, -11.7472, -12.3935]],\n",
       " \n",
       "         [[ 15.4759,  -6.3992,  11.7435,  ...,  -6.2799,  -6.8146,  -6.4048],\n",
       "          [ 13.3054, -49.7845,   4.4584,  ..., -49.5229, -50.0700, -49.6606],\n",
       "          [  9.3230, -42.6298,   0.5528,  ..., -42.3538, -42.9437, -42.5369],\n",
       "          ...,\n",
       "          [  3.7884, -17.8751,   6.2833,  ..., -18.5168, -18.0260, -18.3387],\n",
       "          [  2.8193, -15.6524,   6.6504,  ..., -16.2095, -15.6207, -16.1042],\n",
       "          [  0.9937, -11.9810,   6.8434,  ..., -12.4709, -11.7472, -12.3935]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[[ 1.4492e-02, -8.7142e-02, -2.6076e-03,  ...,  1.9217e-01,\n",
       "           -9.4986e-02, -1.3069e-01],\n",
       "          [-1.0229e-03,  4.9748e-03,  2.5697e-03,  ..., -1.0055e-01,\n",
       "            3.7843e-02,  6.8158e-02],\n",
       "          [-3.5745e-02,  3.8590e-02, -1.0978e-03,  ...,  7.2513e-02,\n",
       "           -1.9204e-02, -4.1570e-02],\n",
       "          ...,\n",
       "          [ 3.1173e-02, -5.6335e-02, -1.3159e-03,  ...,  2.6579e-01,\n",
       "           -1.1221e-01, -1.6387e-01],\n",
       "          [-2.4428e-01,  4.3047e-01,  3.3230e-02,  ..., -2.4506e+00,\n",
       "            1.0549e+00,  1.6614e+00],\n",
       "          [ 1.0881e-02, -2.5195e-02, -1.9580e-03,  ...,  1.4041e-01,\n",
       "           -6.1960e-02, -9.7823e-02]],\n",
       " \n",
       "         [[ 1.4662e-02, -7.3816e-02, -2.3888e-03,  ...,  1.9294e-01,\n",
       "           -9.2459e-02, -1.3103e-01],\n",
       "          [-4.7337e-03,  8.6101e-03,  1.6259e-03,  ..., -9.9206e-02,\n",
       "            3.9631e-02,  6.7194e-02],\n",
       "          [-2.5257e-02,  2.6763e-02, -9.7537e-04,  ...,  7.1480e-02,\n",
       "           -2.3050e-02, -4.3269e-02],\n",
       "          ...,\n",
       "          [ 3.2383e-02, -5.6352e-02, -4.7765e-04,  ...,  2.6868e-01,\n",
       "           -1.1207e-01, -1.6269e-01],\n",
       "          [-2.3938e-01,  4.2979e-01,  3.0403e-02,  ..., -2.4578e+00,\n",
       "            1.0564e+00,  1.6650e+00],\n",
       "          [ 1.1236e-02, -2.5711e-02, -1.8307e-03,  ...,  1.3765e-01,\n",
       "           -6.3153e-02, -1.0016e-01]]], grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(MambaVAE)\n",
    "\n",
    "model = MambaVAE.MambaVAE()\n",
    "model.eval()\n",
    "model(**inputs, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Mamba)\n",
    "import Mamba\n",
    "\n",
    "config = MambaConfig.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "config.use_mambapy = True\n",
    "# config.num_hidden_layers = 12\n",
    "model1 = Mamba.MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", config=config) #.to('cuda')\n",
    "model1.eval()\n",
    "\n",
    "res2 = model1(**inputs, output_ssm_last_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_last_states = res2.ssm_last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = model1(**inputs, inputs_ssm_states=ssm_last_states, output_ssm_last_states = True)\n",
    "# print(tokenizer.batch_decode(res2.logits.argmax(dim=-1)))\n",
    "# print(res2.ssm_last_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(res1.ssm_last_states - res2.ssm_last_states).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x MambaBlock(\n",
       "        (norm): MambaRMSNorm(768, eps=1e-05)\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm(768, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", config=config)\n",
    "model2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model2 = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", config=config)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "print((model2(**inputs).logits - model1(**inputs).logits).abs().sum())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
