{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kLdTjjkkpH7"
   },
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2053,
     "status": "ok",
     "timestamp": 1739729574881,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "PxjGiug1DmSI",
    "outputId": "299f993e-de43-4efa-d4b1-ca2b9253b733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/gdrive')\n",
    "os.chdir(\"/content/gdrive/Othercomputers/retep's PC/MambaVAE\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6120,
     "status": "ok",
     "timestamp": 1739729586214,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "x0z1C7SdkxDw",
    "outputId": "78334f82-beb8-4859-9fff-b3a8cb94c1bc"
   },
   "outputs": [],
   "source": [
    "%pip install causal_conv1d datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHi4PtdekpIU"
   },
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6992,
     "status": "ok",
     "timestamp": 1739730246660,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "bsqZr9NKCXjh",
    "outputId": "a30e890a-f4f0-4ac1-eed3-30342966d74a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, InitMamba, importlib\n",
    "importlib.reload(InitMamba)\n",
    "import VAE\n",
    "importlib.reload(VAE)\n",
    "from VAE import MambaVAE\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<|startoftext|>\"})\n",
    "vae = MambaVAE().cuda().eval()\n",
    "vae.load_state_dict(torch.load('./results/CoT_vae/model.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1739730995105,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "3NTBfcLqCZUm"
   },
   "outputs": [],
   "source": [
    "inputs1 = tokenizer([\"<|startoftext|>According to the given context, as temperature increases particles move faster. Tina's body is at a colder temperature than before, so we can infer that there will be less energy in her body and thus less movement of particles. So slower would be the answer because it matches with our inference from the temperatures mentioned above Answer: slower<|endoftext|>\"], return_tensors='pt').to('cuda')\n",
    "inputs2 = tokenizer(['<|startoftext|>The word \"sad\" is repeated multiple times in the text and it is also combined with other words indicating sadness like \"late\", \"birthday\". Hence, it can be inferred that the underlying emotion of the text is sad. Answer: sad<|endoftext|>'], return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739730997254,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "B7SwM9mKSATx",
    "outputId": "c21171ef-dfd3-435e-bc4f-8ea18eb5a385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1739731001405,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "nJF75eZseUIO",
    "outputId": "f94f2827-daa9-4839-e2c7-7048381f3f2c"
   },
   "outputs": [],
   "source": [
    "res1 = vae(**inputs1)\n",
    "print(res1[:2])\n",
    "print(tokenizer.batch_decode(res1[2].argmax(-1)))\n",
    "\n",
    "res2 = vae(**inputs2)\n",
    "print(res2[:2])\n",
    "print(tokenizer.batch_decode(res2[2].argmax(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3063,
     "status": "ok",
     "timestamp": 1739732321082,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "mBeiZxlPORlI",
    "outputId": "6179696c-2879-49cd-81c0-b647400b0849"
   },
   "outputs": [],
   "source": [
    "r = 0\n",
    "noise = torch.randn_like(res1[3])\n",
    "mix_state = (res1[3]*r+res2[3]*(1-r))\n",
    "tokens = vae.decoder.generate(**tokenizer('<|startoftext|>', return_tensors='pt').to('cuda'),\n",
    "                              inputs_ssm_states = mix_state,\n",
    "                              max_length = 128,\n",
    "                              do_sample=True,\n",
    "                              num_return_sequences=3)\n",
    "tokenizer.batch_decode(tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE38H9XWSAT1"
   },
   "source": [
    "# Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636,
     "referenced_widgets": [
      "08bcf8ece68442e0a1ccb49bcfcfc06e",
      "c7311d6ddc7b46249b9923b7c2963ea0",
      "6b2e3d9010134d1c85f95ddecd4fcbd2",
      "9fc5129e183c45c582aadd3a825c37f0",
      "7810acfbd55f47f2a98f980133eb7360",
      "4946d30497354172a62bc5fb356164b5",
      "0a980b4ac26e42a883acdae50a83044a",
      "c0ab3ac23332492e98ea567c26127351",
      "bc22abda984d43989dcac945d42b1b9f",
      "375391a2526640c89d4de605356fb00c",
      "0460d69cd5054576ac72c00b71e22ef8"
     ]
    },
    "executionInfo": {
     "elapsed": 41537,
     "status": "error",
     "timestamp": 1739730236406,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "3VTScFw6jwCE",
    "outputId": "ed476a26-1172-4d0e-cb5f-cd3ebdb3d622"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bcf8ece68442e0a1ccb49bcfcfc06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-55fa9693350d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state-spaces/mamba-130m-hf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMambaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state-spaces/mamba-130m-hf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./cache/CoT_full.arrow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m   2152\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                         \u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_proc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                     self._download_and_prepare(\n\u001b[0m\u001b[1;32m    925\u001b[0m                         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                         \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 raise OSError(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1739\u001b[0m             \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m                 for job_id, done, content in self._prepare_split_single(\n\u001b[0m\u001b[1;32m   1742\u001b[0m                     \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_prepare_split_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m                 ):\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1852\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m                 \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmax_shard_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bytes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m                         \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/packaged_modules/arrow/arrow.py\u001b[0m in \u001b[0;36m_generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_record_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                         \u001b[0;31m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                         \u001b[0;31m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch, transformers, os\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from InitMamba import MambaForCausalLM\n",
    "from datasets import load_dataset\n",
    "import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = MambaForCausalLM.from_pretrained('state-spaces/mamba-130m-hf').cuda()\n",
    "dataset = load_dataset(\"arrow\", data_files = './cache/CoT_full.arrow', split = 'train').select(range(100000))\n",
    "print(dataset)\n",
    "tot = len(dataset)\n",
    "eval_size= int(tot * 0.05)\n",
    "train_dataset = dataset.select(range(eval_size, tot))\n",
    "eval_dataset = dataset.select(range(eval_size))\n",
    "model.load_state_dict(torch.load(\"results/CoT_full/model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12997,
     "status": "ok",
     "timestamp": 1739672728717,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "y_ZcCjHXY8Lu",
    "outputId": "6b10ff49-9e27-43c3-d1ce-c83f09f4cbc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The 'batch_size' argument of MambaCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose your reply from the options at the end. Does \"The Tucson-Pima County Bicycle Advisory Committee (TPCBAC) serves in an advisory capacity to local governments on issues relating to bicycle recreation, transportation, and safety.\" contain the correct answer to \"What organization advises the Tucson government on bike concerns?\"\n",
      "OPTIONS:\n",
      "- yes\n",
      "- no Thinking: The passage talks about the Tucson-Pima County Bicycle Advisory Committee serving in an advisory capacity to local governments on issues relating to bicycle recreation, transportation and safety. So, it does contain the correct answer for what organization advises the Tucson government on bike concerns. Hence, yes is our choice of reply Answer: yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Choose your reply from the options at the end. Does \"The Tucson-Pima County Bicycle Advisory Committee (TPCBAC) serves in an advisory capacity to local governments on issues relating to bicycle recreation, transportation, and safety.\" contain the correct answer to \"What organization advises the Tucson government on bike concerns?\"\\nOPTIONS:\\n- yes\\n- no Thinking: \\nThe passage talks about the Tucson-Pima County Bicycle Advisory Committee (TPCBAC) serving in an advisory capacity to local governments on issues relating to bicycle recreation, transportation and safety. It does not mention anything about bike concerns or whether it advises the government on these issues. So, the answer is \"no\". Answer: no<|endoftext|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_str = tokenizer.decode(eval_dataset[1]['input_ids'], skip_special_tokens=True)\n",
    "input_ids = tokenizer(input_str.split(' Thinking: ')[0] + ' Thinking: ', return_tensors = 'pt')['input_ids'].cuda()\n",
    "tokens = model.generate(\n",
    "    input_ids,\n",
    "    max_length = 512,\n",
    "    # num_return_sequences = 5,\n",
    "    # do_sample = False,\n",
    "    # num_beams = 5\n",
    "    )\n",
    "print(input_str)\n",
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsx3x4IA0AnB"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 553373,
     "status": "ok",
     "timestamp": 1739730143848,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": 360
    },
    "id": "rZsq3PdNkpIX",
    "outputId": "f8b96f43-362d-471d-cf8d-1a86a6f1baa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739729601.046537    8108 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739729601.060090    8108 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 1466151\n",
      "})\n",
      "The 'batch_size' argument of MambaCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "100% 2291/2291 [08:44<00:00,  4.37it/s]\n",
      "{'eval_loss': 0.14827854931354523, 'eval_model_preparation_time': 0.0044, 'eval_runtime': 524.8114, 'eval_samples_per_second': 139.683, 'eval_steps_per_second': 4.365}\n",
      "-------------------------\n",
      "pred:  The paragraph mentions that the frequency of a wave can be measured by counting the number of crests or compressions that pass through 1 1 second. The greater this count is the higher is the frequency. If fewer crests are coming through per second, then there would be less wave frequency, making \"less\" the correct answer. Answer: less\n",
      "targ:  The paragraph mentions that the frequency of a wave can be measured by counting the number of crests or compressions that pass through in 1 second. The greater this count, the higher is the frequency. If fewer crests are coming through per second, then there would be less wave frequency, making \"less\" the correct answer. Answer: less\n",
      "logits_loss:  0.05878988280892372 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The topic of this question is Kris Humphries, an NBA player. Answer: Kris Humphries\n",
      "targ:  The topic of this question is Kris Humphries, an NBA player. Answer: Kris Humphries\n",
      "logits_loss:  0.0018556645372882485 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The clarification clearly asks whether the user is looking for an instruction manual or not. This statement clarifies the question/query asked by answering it in a different manner. Hence, we can say that the clarification accurately clarifies the query and thus, answer is Yes Answer: Yes\n",
      "targ:  The clarification clearly asks whether the user is looking for an instruction manual or not. This statement clarifies the question/query given by answering it in a different manner. Hence, we can say that the clarification accurately clarifies the query and thus, answer is Yes Answer: Yes\n",
      "logits_loss:  0.05204087123274803 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The given comment does not insult the OP in any way and is just giving him advice on how to adjust to living in a new city. Therefore, it can be classified as \"No\". Answer: No\n",
      "targ:  The given comment does not insult the OP in any way and is just giving him advice on how to adjust to living in a new city. Therefore, it can be classified as \"No\". Answer: No\n",
      "logits_loss:  0.004670480731874704 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Camp Omega is located in West Antarctica. \n",
      "Because of its lower elevation, it's warmer than East Antarctica and experiences more precipitation and weather fronts that extend further into the continent compared to West Antarctica. This makes Camp there last for shorter periods than at Camp Omega, lasting in a colder area with higher elevations. Therefore camp Omega is not experiencing that ice lasts for extended periods, making \"camp\" the answer choice. select. among-D. Answer: Omega\n",
      "targ:  Camp Omega is located in West Antarctica. \n",
      "Because of its lower elevation, it's warmer than East Antarctica and experiences more precipitation and weather fronts that penetrate further into the continent compared to East Antarctica. This makes ice there last for shorter periods than at Camp Alpha, situated in a colder area with higher elevations. Therefore camp Omega is not observing that ice lasts for extended periods, making \"Omega\" the answer choice to select from A-D. Answer: Omega\n",
      "logits_loss:  0.35836708545684814 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The user did not mention any specific cuisine or type of food that they are looking for, and only mentioned that they want a cheap restaurant. Therefore, the answer is 'No'. Answer: No\n",
      "targ:  The user did not mention any specific cuisine or type of food that they are looking for, and only mentioned that they want a cheap restaurant. Therefore, the answer is 'No'. Answer: No\n",
      "logits_loss:  0.0020826084073632956 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The rationale to choose \"gain experience\" as the answer is that: Answer: gain experience\n",
      "targ:  The rationale to choose \"gain experience\" as the answer is that: Answer: gain experience\n",
      "logits_loss:  0.0005299696931615472 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The answer is based on the fact that Brian Cox was a Professor of Particle Physics at The University of Manchester, so he would be expected to work for this university. Answer: University of Manchester\n",
      "targ:  The answer is based on the fact that Brian Cox was a Professor of Particle Physics at The University of Manchester, so he would be expected to work for this university. Answer: University of Manchester\n",
      "logits_loss:  0.00242815725505352 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  We can infer from the last fact in the passage which states \"John travelled to the hallway.\" Answer: hallway\n",
      "targ:  We can infer from the last fact in the passage which states \"John travelled to the hallway.\" Answer: hallway\n",
      "logits_loss:  0.0002901767729781568 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  For action A, not asking for the check does not violate any ethical principles. The person may simply want to spend more time at the restaurant.\\nFor action B, swearing in front of someone old lady can be considered rude and disrespectful to her and therefore violates the ethical of respect for others.\\nTherefore, action B has a greater ethical implication and is considered less ethical than action A. Answer: B\n",
      "targ:  For action A, not asking for the check does not violate any ethical principles. The person may simply want to spend more time at the restaurant.\\nFor action B, swearing in front of an old lady can be considered rude and disrespectful to her and therefore violates the principle of respect for others.\\nTherefore, action B has a greater ethical implication and is considered less ethical than action A. Answer: B\n",
      "logits_loss:  0.09791634231805801 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The answer is 1, because birch does have catkin as one of its parts. Answer: 1\n",
      "targ:  The answer is 1, because birch does have catkin as one of its parts. Answer: 1\n",
      "logits_loss:  0.0023049430456012487 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The story is about a family vacation vacation, which is the main topic and can be encapsulated in the title. The title \"The vacation\" also indicates that there are multiple vacations mentioned in the story. Answer: The vacation\n",
      "targ:  The story is about a family camping vacation, which is the main topic and can be encapsulated in the title. The title \"The vacation\" also indicates that there are multiple vacations mentioned in the story. Answer: The vacation\n",
      "logits_loss:  0.13561157882213593 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The phrase \"open : door\" implies that a door is an affordance of opening. Therefore, the phrase \"sail : boat\" indicates that a boat is an affordance of sailing because sailing and open are similar actions in that they both require manual force to be performed. Answer: boat\n",
      "targ:  The phrase \"open : door\" implies that a door is an affordance of opening. Therefore, the phrase \"sail : boat\" indicates that a boat is an affordance of sailing because sailing and opening are similar actions in that they both require manual force to be performed. Answer: boat\n",
      "logits_loss:  0.0423746295273304 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The food lasts 10 days for 20 days, so it will last one man for 10/10 = 2 days. If there are 40 men in total, then the food will last 40(2) = 80 days, which is 16 times as long as it lasted for 20 men. Since 16 is twice times greater than 20, the food will last 4 times longer and therefore 5 times longer. Answer: c\n",
      "targ:  The food lasts 10 men for 20 days, so it will last one man for 20/10 = 2 days. If there are 40 men in total, then the food will last 40(2) = 80 days, which is 16 times as long as it lasted for 10 men. Since 16 is four times greater than 4, the food will last 4 times longer and therefore 5 days longer. Answer: c\n",
      "logits_loss:  0.19476573169231415 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Middle 1 indicates that Lisa took action to prepare for the quiz by looking over the test booklet, and with that preparation, she succeeded in taking the quiz. Middle 2 does not indicate any action taken by Lisa to prepare for the quiz. Therefore Middle 1 makes more sense. Answer: 1\n",
      "targ:  Middle 1 indicates that Lisa took action to prepare for the quiz by looking over the test booklet, and with that preparation, she succeeded in taking the quiz. Middle 2 does not indicate any action taken by Lisa to prepare for her quiz. Therefore Middle 1 makes more sense. Answer: 1\n",
      "logits_loss:  0.056585893034935 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The first sentence is about Ollie carrying Tommy up the steps and not about himself. Hence, it cannot be used to conclude that \"Ollie d legs dangled\" Answer: no\n",
      "targ:  The first sentence is about Ollie carrying Tommy up the steps and not about himself. Hence, it cannot be used to conclude that \"Ollie's legs dangled\" Answer: no\n",
      "logits_loss:  0.03700621426105499 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The comment text is a political statement about China destroying the country. This content does not promote or support equality, diversity or inclusion. Therefore, the answer is 'Not Hope Speech'. Answer: Not Hope Speech\n",
      "targ:  The comment text is a political statement about China destroying the country. This content does not promote or support equality, diversity or inclusion. Therefore, the answer is 'Not Hope Speech'. Answer: Not Hope Speech\n",
      "logits_loss:  0.009421360678970814 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Here's an direct answer: \"When the Lights Go Down in the City\" is a song by American rock band Journey. It was written for and featured on their seventh album Infinity, released as its first single that same year. Answer: Journey\n",
      "targ:  Here's an direct answer: \"When the Lights Go Down in the City\" is a song by American rock band Journey. It was written for and featured on their seventh album Infinity, released as its first single that same year. Answer: Journey\n",
      "logits_loss:  0.03631141781806946 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The question asks the reader to identify how many orders of mendicant friars emerged during the Middle Ages.\n",
      "This information can be found in context, which states that \"two orders of mendicant friars emerged were established at this time: one led by St. Francis and another by St. Dominic Answer: two\n",
      "targ:  The question asks the reader to identify how many orders of mendicant friars emerged during the Middle Ages.\n",
      "This information can be found in context, which states that \"two orders of mendicant friars\" were established at this time: one led by St. Francis and another by St. Dominic Answer: two\n",
      "logits_loss:  0.045657504349946976 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The sentence states that the biggest marque d'attention a tourist can receive is to be invited to a Creole home. This implies that it is not something common or ordinary and hence, indicates that the hypothesis of \"Being invited to a Creole home is a common, everyday occurrence\" does not follow from this premise. Answer: no\n",
      "targ:  The sentence states that the biggest marque d'attention a tourist can receive is to be invited to a Creole home. This implies that it is not something common or ordinary and hence, indicates that the hypothesis of \"Being invited to a Creole home is a common, everyday occurrence\" does not follow from this premise. Answer: no\n",
      "logits_loss:  0.02259695529937744 ; kl_loss:  0.0\n",
      "###################################################\n",
      "-------------------------\n",
      "pred:  The proposed mission to Neptune is not mentioned. The only mention of the planet in this excerpt is a note that there have been proposals for such missions, but they are very distant and probably will never happen. Answer: no\n",
      "targ:  The proposed mission to Neptune is not mentioned. The only mention of the planet in this excerpt is a note that there have been proposals for such missions, but they are very distant and probably will never happen. Answer: no\n",
      "logits_loss:  0.010984842665493488 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The passage talks about the Tucson-Bima Countyicyicycle Advisory Committee serving in an advisory capacity on local governments on issues relating to bicycle safety, transportation and safety. So, it does contain the correct answer for what organization advises the Tucson- on bicy issues. Hence, yes is our choice of reply Answer: yes\n",
      "targ:  The passage talks about the Tucson-Pima County Bicycle Advisory Committee serving in an advisory capacity to local governments on issues relating to bicycle recreation, transportation and safety. So, it does contain the correct answer for what organization advises the Tucson government on bike concerns. Hence, yes is our choice of reply Answer: yes\n",
      "logits_loss:  0.3260944187641144 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The system generated reference is grammatically correct and it is a valid response. Therefore, the answer is 1. Answer: 1\n",
      "targ:  The system generated reference is grammatically correct and it is a valid response. Therefore, the answer is 1. Answer: 1\n",
      "logits_loss:  0.00022310482745524496 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The premise describes a situation where there is a person on the beach in a green hang glider. The second sentence only states that there is a person on the beach, without providing any information about whether they are standing or sitting etc etc., so it does not contradict the first sentence and hence can be assumed to be true. Therefore, answer: yes Answer: yes\n",
      "targ:  The premise describes a situation where there is a person on the beach in a green hang glider. The second sentence only states that there is a person on the beach, without providing any information about whether they are standing or sitting down etc., so it does not contradict the first sentence and hence can be assumed to be true. Therefore, answer: yes Answer: yes\n",
      "logits_loss:  0.032023970037698746 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The rationale is: \"C, expected because it was not what he thoughtshe thought would happen\".\n",
      "\"\" Answer: C\n",
      "targ:  The rationale is: \"C, expected because it was not what he/she thought would happen\".\n",
      "\"\" Answer: C\n",
      "logits_loss:  0.1394975483417511 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The minimal text span containing the date of birth is \"January 24, 1917\". Answer: January 24, 1917\n",
      "targ:  The minimal text span containing the date of birth is \"January 24, 1917\". Answer: January 24, 1917\n",
      "logits_loss:  0.00041046904516406357 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The passage states that the average reach correlates to a person's height. It also mentions that arm and sex have to be taken into account when determining someone's height span their arm width, but it does not suggest any reason why your arm span would ever be different than your height. Therefore, you can infer that this context that an arm's arm width is not likely than not to to his/her height. The answer must therefore be No. Answer: Yes\n",
      "targ:  The passage states that the average reach correlates to a person's height. It also mentions that age and sex have to be taken into account when determining someone's height from their arm span, but it does not suggest any reason why your arm width would ever be different than your height. Therefore, you can infer given this context that an individual's arm width is more likely than not equal to his/her height. The answer must therefore be Yes. Answer: Yes\n",
      "logits_loss:  0.23694314062595367 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  This passage does not provide any information about the president who announced the new CIA director. The answer to this question would be \"it is not to say\", but since that option was not provided in this example, we can conclude that the correct answer should be \"no\". Answer: no\n",
      "targ:  This passage does not provide any information about the president who announced the new CIA director. The answer to this question would be \"it is impossible to say\", but since that option was not provided in this example, we can conclude that the correct answer should be \"no\". Answer: no\n",
      "logits_loss:  0.018117614090442657 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The question asks how old the goretender was, who Reto Berra replaced in his NHL debut. In the passage, the paragraph that indicates that Pickard replaced \"ra mentions \"he had to go in for Reto Berra who was injured in a collision.\" Therefore, c. is the correct answer. Answer: c\n",
      "targ:  The question asks how old the goaltender was, who Reto Berra replaced in his NHL debut. In the passage, the paragraph that indicates that Pickard replaced Berra mentions \"he had to come in for Reto Berra who was injured in a collision.\" Therefore, c. is the correct answer. Answer: c\n",
      "logits_loss:  0.17849740386009216 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Tori was 4 feet2 feet tall, then she grew 2.8 feet taller, so now she is 4.4 + 2.8 = 7.2 feet tall Answer: 7.2\n",
      "targ:  Tori was 4.4 feet tall, then she grew 2.8 feet taller, so now she is 4.4 + 2.8 = 7.2 feet tall Answer: 7.2\n",
      "logits_loss:  0.06801678240299225 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The first dialogue clearly represents a customer's query and the second dialogue is part of the same context. Therefore, the speaker of this dialogue dialogue is also a customer.\" Answer: customer\n",
      "targ:  The first dialogue clearly represents a customer's query and the given dialogue is part of the same context. Therefore, the speaker of this given dialogue is also a customer.\" Answer: customer\n",
      "logits_loss:  0.12200786173343658 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  According to the given context, as temperature increases particles move faster. Tina's body is at a colder temperature than before, so we can infer that there will be less energy in her body and thus less movement of particles.\n",
      "So slower would be the answer Answer it matches with our inference from the temperatures mentioned above Answer: slower\n",
      "targ:  According to the given context, as temperature increases particles move faster. Tina's body is at a colder temperature than before, so we can infer that there will be less energy in her body and thus less movement of particles.\n",
      "So slower would be the answer because it matches with our inference from the temperatures mentioned above Answer: slower\n",
      "logits_loss:  0.025790825486183167 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The question is asking for the first name of a person that Tom would never see again. \n",
      "In this context, it can be inferred that the answer will refer to someone who had a relationship with Tom in his past and whom he does not intend on seeing again in his future. \n",
      "Based on these criteria, there are only two candidates: Maria or Betsy (his wife). However, since Betsy cannot fit into criteria given above (since he have married married), then correct choice must be be \". Answer: Maria\n",
      "targ:  The question is asking for the first name of a person that Tom would never see again. \n",
      "In this context, it can be inferred that the answer will refer to someone who had a relationship with Tom in his past and whom he does not intend on seeing again in his future. \n",
      "Based on these assumptions, there are only two candidates: Maria or Betsy (his wife). However, since Betsy cannot fit both criteria given earlier (since they have been married), the correct choice must then be Maria. Answer: Maria\n",
      "logits_loss:  0.2590024769306183 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  In the conversation provided, ASSISTANT asks the user about their preference of movies and then recommends a particular movie based on the user's preferences. In this, in this example, ASSISTANT seems to be making recommendations to the USER. Typically assistants make recommendations or request information from users so it is more likely that ASSISTANT is making a recommendation than the USER. for information from another user. Answer: ASSISTANT\n",
      "targ:  In the conversation provided, ASSISTANT asks the user about their preference of movies and then recommends a particular movie based on the user's preferences. In fact, in this example, ASSISTANT seems to be making recommendations to the USER. Usually assistants make recommendations or request information from users so it is more likely that ASSISTANT is making a recommendation than the USER asking for information from another user. Answer: ASSISTANT\n",
      "logits_loss:  0.10338394343852997 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Here, the user is discussing the options for scheduling an event. Answer: user\n",
      "targ:  Here, the user is discussing the options for scheduling an event. Answer: user\n",
      "logits_loss:  0.0012049429351463914 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The sentence describes how the families carved out for themselves large regions (fiefdoms) became became powerful. This is a stationary event, so we could just choose \"yes\" as the answer because it would be true 100 years later. Answer: yes\n",
      "targ:  The sentence describes how the families carved out for themselves large regions (fiefdoms) and became powerful. This is a stationary event, so we could just choose \"yes\" as the answer because it would be true 100 years later. Answer: yes\n",
      "logits_loss:  0.05359826609492302 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Selectors and vascular adhesionins adhesion are cell adhesion adhesion molecules. They mediate the first phase of leukocyte or adhesion adhesion but do not support adhesion adhesion to the endothelium. Hence, option 2 is correct and the remaining options are incorrect. Answer: 2\n",
      "targ:  Selectins and vascular targeins are cell surface adhesion molecules. They mediate the first phase of leukocyte / endothelium adhesion but do not support firm adhesion to the endothelium. Hence, option 2 is correct and the remaining options are incorrect. Answer: 2\n",
      "logits_loss:  0.7449134588241577 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The generated sentence \"You are likely to name a snake in the nest\" is semantically related to the input. It has high commonsense plausibility, that is to be reasonable probability of it being true. Answer: You are likely to name a snake in the nest\n",
      "targ:  The generated sentence \"You are likely to name a snake in the nest\" is semantically related to the input. It has high commonsense plausibility, that is to have reasonable probability of it being true. Answer: You are likely to name a snake in the nest\n",
      "logits_loss:  0.04154599457979202 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  Here's a question for the given article:\\n\\nQuestion: How many more more drove alone instead of carpooling? Answer: How many percent more drove alone instead of carpooling?\n",
      "targ:  Here's a question for the given article:\\n\\nQuestion: How many percent more drove alone instead of carpooling? Answer: How many percent more drove alone instead of carpooling?\n",
      "logits_loss:  0.03360326960682869 ; kl_loss:  0.0\n",
      "-------------------------\n",
      "pred:  The headline is about a movie festival, which is an entertainment-related event. Therefore, the article belongs to Entertainment category. Answer: Entertainment\n",
      "targ:  The headline is about a movie festival, which is an entertainment-related event. Therefore, the article belongs to Entertainment category. Answer: Entertainment\n",
      "logits_loss:  0.0012338069500401616 ; kl_loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "!python Trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLobtlCAz-hd"
   },
   "outputs": [],
   "source": [
    "!python Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BWvknpyjuQc"
   },
   "outputs": [],
   "source": [
    "!python CoT_full_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD55Rw9XCjDE"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_kLdTjjkkpH7",
    "VHi4PtdekpIU",
    "TE38H9XWSAT1"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0460d69cd5054576ac72c00b71e22ef8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08bcf8ece68442e0a1ccb49bcfcfc06e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7311d6ddc7b46249b9923b7c2963ea0",
       "IPY_MODEL_6b2e3d9010134d1c85f95ddecd4fcbd2",
       "IPY_MODEL_9fc5129e183c45c582aadd3a825c37f0"
      ],
      "layout": "IPY_MODEL_7810acfbd55f47f2a98f980133eb7360"
     }
    },
    "0a980b4ac26e42a883acdae50a83044a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "375391a2526640c89d4de605356fb00c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4946d30497354172a62bc5fb356164b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b2e3d9010134d1c85f95ddecd4fcbd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0ab3ac23332492e98ea567c26127351",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc22abda984d43989dcac945d42b1b9f",
      "value": 1
     }
    },
    "7810acfbd55f47f2a98f980133eb7360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc5129e183c45c582aadd3a825c37f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_375391a2526640c89d4de605356fb00c",
      "placeholder": "​",
      "style": "IPY_MODEL_0460d69cd5054576ac72c00b71e22ef8",
      "value": " 431000/0 [00:26&lt;00:00, 28917.04 examples/s]"
     }
    },
    "bc22abda984d43989dcac945d42b1b9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c0ab3ac23332492e98ea567c26127351": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c7311d6ddc7b46249b9923b7c2963ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4946d30497354172a62bc5fb356164b5",
      "placeholder": "​",
      "style": "IPY_MODEL_0a980b4ac26e42a883acdae50a83044a",
      "value": "Generating train split: "
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
